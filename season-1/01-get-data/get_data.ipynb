{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tD-GvSpBxCDj"
      },
      "source": [
        "# #1. Get Cannabis Data\n",
        "\n",
        "Hello everyone, my name is Keegan and its awesome to meet each of you. This group is for people who want to apply data science to cannabis data. My background is in economics and lab work, so I have an interesting perspective and abundant interest in the subject, so, I wanted to host a group. I can share some of my work, but this group can go where the group decides, so, please feel free to speak and share. And we'll take it from there."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "91oJq77cyui_"
      },
      "source": [
        "## What does a data scientist do?\n",
        "A senior analyst may spend the bulk of their time cleaning data to be used in prediction models. The data is presented in a final report in a simple format. The data sets may be in the thousands to in the billions.\n",
        "\n",
        "1.\tThe first step is to find or build a dictionary of the data.\n",
        "2.\tNext, you need to import all of the data so that you can work with it.\n",
        "3.\tNow, you will need to begin to clean the data. This requires an understanding of the structure of the data.\n",
        "4.\tOnce the data is defined, read, and formatted, then you can begin analysis. This is typically one of the faster stages if data is cleaned and formatted well. Data is then analyzed with mathematical and statistical functions, such as regression analysis, to make comparisons and gain insights.\n",
        "\n",
        "The first rule of data science: look at the data. Next, you need to formulate a research question. Then you can begin to think about how to clean the data and what variables to keep. Data scientists may use timeseries regressions. You can find interesting points in time and compare the series from before and after the event to try to identify any structural breaks. You can control for region, such as urban or rural, and other geographical variables. It is often incredibly fruitful to combine multiple datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Rylpdsb21Wo_"
      },
      "source": [
        "## Reading the Data\n",
        "Examples of way to work with Washington State cannabis traceability data.\n",
        "<!--The first step of the pipeline with any data science related tutorial is usually the data loading component. Besides visually describing the dataset in use to your audience, also try to briefly explain (in one or two sentences) where the data came from, i.e., the source of the data. Other specifications like dimensions and attribute type are important but can be neatly explained with examples using code and tools such as `pandas`.-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "g1_euxvq1XQZ"
      },
      "outputs": [],
      "source": [
        "# Standard imports.\n",
        "import datetime\n",
        "\n",
        "# External imports.\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.graphics.regressionplots import abline_plot\n",
        "\n",
        "\n",
        "# Read in lab results from the .csv file obtained through public\n",
        "# records request.\n",
        "file_name = 'LabResults_0.csv'\n",
        "data = pd.read_csv(\n",
        "    file_name,\n",
        "    encoding='utf-16',\n",
        "    sep='\\t',\n",
        ")\n",
        "print('Number of observations:', len(data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ia6remU_fV7W"
      },
      "source": [
        "## Exploring the Data\n",
        "Since you are teaching through writing and not actually live coding, resist the temptation to write code that does anything with the data like transformation or feature engineering before actually exploring it. It's a common mistake or practice that should be minimized. You want to give the readers some idea about the data through basic statistics, plots, and figures. Practise this as much as you can, and it will become an important habit in your data science work flow. Your readers will also appreciate the courtesy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count the number of observations.\n",
        "print('Number of observations:', len(data))\n",
        "\n",
        "# Look at the data\n",
        "obs = data.iloc[0].to_dict()\n",
        "print(list(data.columns))\n",
        "\n",
        "# Sum tests per day\n",
        "LIMIT = 10000\n",
        "sample = data[-LIMIT:]\n",
        "print(list(sample.iloc[0].keys()))\n",
        "print(list(sample['intermediate_type'].unique()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MwVV-MGG14UL"
      },
      "source": [
        "## Preprocessing the Data\n",
        "Although sometimes not necessary, as some datasets already come preprocessed, I believe it is important to slightly mention what type of preprocessing steps the data has undergone -- even if you need to do this through code examples. It should clarify any confusion that can present itself during the modeling section of the tutorial. Remember, your audience wants to get a broad understanding of the data before the modeling component of the tutorial, so try to explain this part of the tutorial as clear as possible with examples. Take advantage of your notebook features and other tools such as `matplotlib` and `pandas`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "05S4Z52q156M"
      },
      "outputs": [],
      "source": [
        "# Create some variables\n",
        "sample['time'] = pd.to_datetime(sample['tested_at'])\n",
        "sample['lab'] = sample['global_id'].str.slice(2, 4)\n",
        "print('Number of labs:', len(sample['lab'].unique()))\n",
        "\n",
        "# Estimate test per day in February\n",
        "flower = sample.loc[sample['intermediate_type'] == 'flower_lots']\n",
        "\n",
        "# Perform some simple statistics.\n",
        "high_thc = flower.loc[(flower['cannabinoid_d9_thca_percent'] <= 35) & \n",
        "                      (flower['cannabinoid_d9_thca_percent'] > 20) &\n",
        "                      (flower['cannabinoid_cbda_percent'] < 0.5) ]\n",
        "\n",
        "\n",
        "high_cbd = flower.loc[flower['cannabinoid_cbda_percent'] > 5]\n",
        "high_cbd['moisture_content_water_activity_rate'].plot()\n",
        "\n",
        "# Define independent and dependent variables for a regression.\n",
        "X = high_thc[['cannabinoid_cbda_percent',]].fillna(0)\n",
        "Y = high_thc['cannabinoid_d9_thca_percent'].fillna(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uZ6eXpGl2WBt"
      },
      "source": [
        "## Modeling the Data\n",
        "If you are using tools such as PyTorch or TensorFlow for your data science projects, this section is reserved for the computation graph. Here you usually just state very briefly what you are building. No need to go into details just yet!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "l7tEeylE2kSR"
      },
      "outputs": [],
      "source": [
        "# Fit a regression model.\n",
        "X = sm.add_constant(X)\n",
        "model = sm.OLS(Y, X)\n",
        "regression_results = model.fit()\n",
        "print(regression_results.summary())\n",
        "\n",
        "# Plot the regression\n",
        "ax = high_thc.plot(\n",
        "    x='cannabinoid_cbda_percent',\n",
        "    y='cannabinoid_d9_thca_percent',\n",
        "    kind='scatter'\n",
        ")\n",
        "abline_plot(model_results=regression_results, ax=ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GGGakX-l2o0s"
      },
      "source": [
        "## Testing the Model\n",
        "One of the things I have learned over the years is that everything in data science is better understood with examples, rather than just using plain code or pictures. Before you begin training your models make sure to explain to the reader what the model is expecting as input and what it is expected to output. Rendering code here with nice descriptions help to prepare the reader on what to expect during training the model, especially since the training code is usually longer than most sections of the tutorial. With libraries like [PyTorch](https://pytorch.org/) and [DyNet](http://dynet.io/) this is fairly easy since they are dynamic computing libraries. TensorFlow also offers an [eager](https://www.tensorflow.org/guide/eager) execution command, `tf.enable_eager_execution()` to evaluate operations immediately. This is what's called imperative programming and I am glad they have it. It makes it easy to teach others about the beautiful things these tools are able to accomplish. I like to think that data science is about storytelling and discovery, and it should remain that way. Clear writing helps!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Trend an analyte (butane) over time.\n",
        "concentrate_types = [\n",
        "    'hydrocarbon_concentrate',\n",
        "    'concentrate_for_inhalation',\n",
        "    'non-solvent_based_concentrate',\n",
        "    'co2_concentrate',\n",
        "    'food_grade_solvent_concentrate',\n",
        "    'ethanol_concentrate',\n",
        "]\n",
        "concentrates = sample.loc[sample['intermediate_type'].isin(concentrate_types)]\n",
        "\n",
        "# Aggregate data by day.\n",
        "daily_concentrates = concentrates.groupby(concentrates.time.dt.date).mean()\n",
        "daily_concentrates = daily_concentrates.loc[daily_concentrates.index > pd.to_datetime('2020-12-01')]\n",
        "\n",
        "# Look at the data!\n",
        "fig, ax = plt.subplots(1, 1)\n",
        "fig.set_size_inches(9, 5)\n",
        "ax.plot(daily_concentrates.index, daily_concentrates.solvent_butanes_ppm)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NtF8ipN93JTj"
      },
      "source": [
        "## Training the Model\n",
        "When training the models you would specify what kind of optimization, hyperparameters, and data iterating methods you are using. To be honest, the training code is usually self-explanatory. If you did your job at the beginning, explaining your dataset and testing the model, this part of the tutorial is probably the one that needs less explanation. In my experience, most data computing libraries use similar training strategies, thus the training structure has become ubiquitous in some sense. If there is still any clarification in your training that you need the reader to know, you can always explain it beforehand. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "P2b6vtcU3ddJ"
      },
      "outputs": [],
      "source": [
        "# Fit a trend line.\n",
        "X = daily_concentrates.index.map(datetime.date.toordinal)\n",
        "Y = daily_concentrates['solvent_butanes_ppm'].fillna(0).values\n",
        "X = sm.add_constant(X)\n",
        "model = sm.OLS(Y, X)\n",
        "results = model.fit()\n",
        "print(results.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C3Hf7M1r3isG"
      },
      "source": [
        "## Evaluating the Model\n",
        "And lastly, it is  good practice to evaluate your models on some held out samples of the dataset. This helps the reader to get a gist of what the tutorial you just showed him/her contains. It also helps to re-emphasize on the values the tutorial is providing for the reader. This part of the tutorial also helps to finalize your final thoughts and share insights with your readers. Readers love insights. You can share plots, a lot of examples, and even explore the parameters of the model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "GiRH0DTd3u4N"
      },
      "outputs": [],
      "source": [
        "# Plot the trend line with the daily data points.\n",
        "ax.plot(daily_concentrates.index, results.fittedvalues, c='r')\n",
        "ax.set_ylabel('ppm')\n",
        "ax.set_title('Average Butane levels in WA Concentrates', fontsize=18)\n",
        "fig.autofmt_xdate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "plZ38XRC3zJu"
      },
      "source": [
        "## Final Thoughts\n",
        "You are not writing a book, so it is not necessary to have a conclusion section. In my experience, you use the final section to summarize all your findings and the future ideas you are working on. This is also a great time to congratualte the reader for making it to the end of the tutorial -- that's a huge achievement. You show that you appreciate the readers. Then you can end the section with your favorite quote. \n",
        "\n",
        "And that's it! Congratulations for reaching the end of this primer. You are now more than equipped to deliver excellent tutorials to the whole data science community and to a wider audience. With this short primer, you should reach thousands, and hopefully millions, but most importantly, with it, you should be able to bring value to your readers and keep expanding the human knowledge base. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ng8pGkkw8XB3"
      },
      "source": [
        "## References\n",
        "\n",
        "We thank [The Cannabis Observer](https://cannabis.observer/) for diligently requesting public Washington State cannabis traceability data records.\n",
        "\n",
        "### Data Sources\n",
        "\n",
        "- [Cannabis Genome Data](https://www.kaggle.com/paultimothymooney/how-to-query-the-1000-cannabis-genomes-project)\n",
        "- [WSLCB December 2020 Data](https://lcb.app.box.com/s/fnku9nr22dhx04f6o646xv6ad6fswfy9?page=1)\n",
        "\n",
        "### Resources\n",
        "\n",
        "- [Add footnote under the x-axis using matplotlib](https://stackoverflow.com/questions/7917107/add-footnote-under-the-x-axis-using-matplotlib)\n",
        "- [Data Analysis with Pandas Blog Series](https://hackersandslackers.com/series/data-analysis-pandas/)\n",
        "- [Data Visualization With Seaborn and Pandas](https://hackersandslackers.com/plotting-data-seaborn-pandas/)\n",
        "- [How to build a regression model in python?](https://stackoverflow.com/questions/44325017/how-to-build-a-regression-model-in-python)\n",
        "- [How to parse tsv file with python?](https://stackoverflow.com/questions/42358259/how-to-parse-tsv-file-with-python)\n",
        "- [How to plot statsmodels linear regression (OLS) cleanly](https://stackoverflow.com/questions/42261976/how-to-plot-statsmodels-linear-regression-ols-cleanly)\n",
        "- [Linear Models](https://scikit-learn.org/stable/modules/linear_model.html)\n",
        "- [Python Pandas Error tokenizing data](https://stackoverflow.com/questions/18039057/python-pandas-error-tokenizing-data)\n",
        "- [Rotate axis text in python matplotlib](https://stackoverflow.com/questions/10998621/rotate-axis-text-in-python-matplotlib)\n",
        "- [Select rows from a DataFrame based on multiple values in a column in pandas](https://stackoverflow.com/questions/36410075/select-rows-from-a-dataframe-based-on-multiple-values-in-a-column-in-pandas)\n",
        "- [Summing the number of occurrences per day pandas](https://stackoverflow.com/questions/17706109/summing-the-number-of-occurrences-per-day-pandas)\n",
        "- [UnicodeDecodeError when reading CSV file in Pandas with Python](https://stackoverflow.com/questions/18171739/unicodedecodeerror-when-reading-csv-file-in-pandas-with-python)\n",
        "- [WAC 314-55-102](https://apps.leg.wa.gov/wac/default.aspx?cite=314-55-102)\n",
        "- [WSLCB How to Make a Public Records Request](https://lcb.wa.gov/records/make-public-records-request)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vvQvwkAx4u5r"
      },
      "source": [
        "Written with ❤️ by [The Cannabis Data Science Team](https://cannlytics.com/team)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "name": "Writing Primer for Data Scientists.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('cds')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.12 | packaged by conda-forge | (main, Mar 24 2022, 23:18:12) [MSC v.1929 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "1d2303973efe03ed9fcd792b92b1518d948a0b1aa22ff91cb505006347db75c3"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
